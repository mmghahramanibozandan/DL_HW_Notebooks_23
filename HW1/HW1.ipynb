{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","source":["#**Deep Learning Homework 1: *from the Perceptron to DNN***\n","### MSc Computer Science, Data Science, Cybersecurity @UNIPD\n","### 2nd semester - 6 ECTS\n","### Prof. Alessandro Sperduti, Prof. Nicolò Navarin and Dr. Luca Pasa\n","---"],"metadata":{"id":"SY5WztYNneGg"}},{"cell_type":"markdown","metadata":{"id":"d8MTQQX3WaFa"},"source":["In this first homework, we are going to write our own simple feedforward neural network using `Python` and `NumPy` (the standard numeric library for Python). We will start by implementing just a simple neuron, or perceptron, then we define the training algorithm for this simple model.\n","The second part consists in defining a simple neural network to perform digits classification."]},{"cell_type":"markdown","source":["##**Important Instructions for Submissions:**\n","\n","Generally, in the homeworks, you will be either required to complete a part of Python code or to answer questions in text cells. Code and text cells where you are expected to write your answers have been marked by `%STARTCODE` and `%ENDCODE` or `%STARTEXT` and `%ENDTEXT` tags, respectively. Note that you should never change, move or remove these two tags, otherwise your answers will be __not__ valid. As you will see in this notebook, each cell that includes a `[TO COMPLETE]` part has been put between these placeholders. \n","\n","As an example, if the task is to _\"define a variable named `x` and assign it to number 2\"_, the following answer style is presented:"],"metadata":{"id":"EnhjgoafUY69"}},{"cell_type":"markdown","source":["`%STARTCODE`"],"metadata":{"id":"1F8KAFuXVp_Y"}},{"cell_type":"code","source":["x = 2 #[TO COMPLETE]\n"],"metadata":{"id":"UrBK7IsgUozD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"o3g-jmzIDnm5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`%ENDCODE`"],"metadata":{"id":"DU5RrnqiVsk9"}},{"cell_type":"markdown","source":["Similarly, if the task is a question-answering one, the same style is used. As an instance, the proper answer to the question \"How can we import the numpy library in python?\", must seem like the following:"],"metadata":{"id":"GcAjL4TOWEbn"}},{"cell_type":"markdown","source":["`%STARTEXT`"],"metadata":{"id":"dvKdI4PBWf4W"}},{"cell_type":"markdown","source":["Answer: **[TO COMLPETE]**\n","\n","In order to do so, we use the keyword `import`, accompanied by the name of the library we would like to add to our notebook."],"metadata":{"id":"BylXZkvyWk1c"}},{"cell_type":"markdown","source":["`%ENDTEXT`"],"metadata":{"id":"uA74CJWmWhtM"}},{"cell_type":"markdown","source":["As already mentioned, all `%START` and `%END` keywords have already been placed and you just need to be careful not to delete, move or change them. Now let's start with the first homework!"],"metadata":{"id":"fmaJhiPFXaXv"}},{"cell_type":"markdown","metadata":{"id":"-bEFm73cYFQy"},"source":["## Exercise 1.1: Perceptron\n","\n","In this first exercise, we will implement a simple neuron, or perceptron, as described below. We will have just three inputs and one output neuron (we omit the bias term for now).\n","Notice how the perceptron simply performs a sum of the individual inputs multiplied by the corresponding weights mapped through an activation function $\\sigma(\\cdot)$.  This can also be expressed as a dot product of the weight vector $\\textbf{W}$ and the input vector $\\textbf{x}$, thus: $$\\hat{y}=\\sigma(\\textbf{W}^T \\textbf{x})$$"]},{"cell_type":"markdown","metadata":{"id":"hDlidWmiYuKB"},"source":["We will begin by implementing the perpetron by using the [numpy](https://docs.scipy.org/doc/numpy/reference/) library:"]},{"cell_type":"code","metadata":{"id":"i0UCur_TYckH"},"source":["import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JQONq1k6Y1Zx"},"source":["### Training data\n","\n","Let's consider a very simple dataset. The dataset is made of four input vectors $\\textbf{x} \\in \\mathbb{R}^3$ and the corresponding desired target values $y$. In the table below, each row is a single sample; the first three columns are the input vector components, whereas the last column is the target output.\n","\n","||Input $x_i$||Output $y$|\n","|:----:|:---:|:---:|---:|\n","| 1    | 1   | 0   | 1  |\n","| 1    | 0   | 0   | 1  |\n","| 0    | 1   | 0   | 0  |\n","| 0    | 0   | 0   | 0  |\n","\n","Notice that our target outputs are equal to the first component of the input, therefore the task that the model should learn is very simple. We will see how the perceptron is able to learn that starting from this toy dataset.\n","\n","Now let's define the `X` and `y` matrices:"]},{"cell_type":"code","metadata":{"id":"u1FzAUxhY9PA"},"source":["# Our input data is a matrix, each row is one input sample\n","X = np.array([[1,1,0],\n","              [1,0,0],\n","              [0,1,0],\n","              [0,0,0]])\n","    \n","# The target output is a column vector in 2-D array format (.T means transpose)\n","y = np.array([[1,1,0,0]]).T\n","\n","print('X =',X)\n","print('y =',y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z-W8wMnoZH71"},"source":["### Activation function\n","\n","As we said before, in order to define a perceptron we need to define the activation function $f(\\cdot)$. There are many possibile activation function that can be used, let's plot some of the most common ones:\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"Fi5ZK9zKZssu"},"source":["import matplotlib.pyplot as plt \n","\n","x = np.arange(-4,4,.01)\n","plt.figure()\n","plt.plot(x, np.maximum(x,0), label='ReLu')\n","plt.plot(x, 1/(1+np.exp(-x)), label='Sigmoid')\n","plt.plot(x, np.tanh(x), label='tanh')\n","plt.axis([-4, 4, -1.1, 1.1])\n","plt.title('Some Activation Functions')\n","plt.grid(True)\n","l = plt.legend()\n","plt.show()\n","\n","# Delete temporary variables, so not to cause any confusion later :-)\n","del x, l"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jdmJ6-qaZjuZ"},"source":["In this particular exercise we will use the sigmoid function. So let's define $f(\\cdot)$ as the sigmoid function\n","\n","$$\\sigma(x)=\\frac{1}{1+\\exp^{-x}}$$"]},{"cell_type":"code","metadata":{"id":"wwnk5RgKZRox"},"source":["def sigma(x):\n","    # Sigmoid function\n","    return 1 / ( 1 + np.exp(-x) )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vX-62n3Io0XU"},"source":["### Weight initialization\n","\n","Now we have to initialise the weights. Let's initialize them randomly, so that their mean is zero. The weights matrix maps the input space into the output space, therefore in our case $\\mathbf{W} \\in \\mathbb{R}^{3 \\times 1}$"]},{"cell_type":"code","metadata":{"id":"9f83YkaNtd5h"},"source":["# fix random seed for reproducibility\n","np.random.seed([42])\n","\n","# initialize weights randomly with zero mean and uniformly distributed values in [-1,1]\n","W = 2 * np.random.random((3,1)) - 1\n","\n","print('W =', W)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pZd-aQxAuX49"},"source":["### Forward propagation\n","\n","Next, let's try to implement one round of forward propagation.  This means taking an input sample and moving it forward through the network, calculating the output of the network eventually.\n","\n","For our single neuron this is simply $\\hat{\\mathbf{y}} = \\sigma(\\mathbf{W}^T \\mathbf{x})$, where $\\mathbf{x}$ is one input vector.\n","\n","Each input sample is arranged as a row of the matrix `X`, therefore we can access the first row by `X[0]`. Let's store it in the variable `X0` for easier access. We'll use `reshape` to make sure it's expressed as a column vector."]},{"cell_type":"code","metadata":{"id":"9gTx4QxUumvQ"},"source":["X0 = np.reshape(X[0], (3,1))\n","print(\"X0 =\",X0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IpDCnmxWurjC"},"source":["The output $\\hat{y}$ for the first input can be calculated according to the formula given above"]},{"cell_type":"code","metadata":{"id":"idDZfTVKuyPf"},"source":["y_out = sigma(np.dot(W.T, X0))\n","\n","print('y_out =', y_out)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Oda3Y3btu8Uz"},"source":["the target result is stored in `y[0]`.  If you check back, you can see we defined it to be $y_0=0$. You can see that our network is pretty far away from the right answer... this is why we need to backpropagate the error, to adjust the weights in the right direction!"]},{"cell_type":"markdown","metadata":{"id":"oGvgZCDFu-EM"},"source":["### Backpropagation\n","\n","The following step is updating the weights by propagating the error backwards in the network.  How this is done depends on the activation function, and namely on its derivative. The activation function of the considered model is the sigmoid, and its derivative is:\n","\n","$$\\sigma(x)'=\\sigma(x) \\cdot (1-\\sigma(x))$$\n","\n","Recall that the weight update in genereal is given as $\\Delta w_{ji} = -\\epsilon \\delta_j x_i$.\n","Our network has only one layer, so $x_i$ is just the input $\\mathbf{x}$, and a single output neuron so there is no actual need for index $j$. \n","\n","In matrix form we can calculate this for all the weights:\n","\n","$$\\Delta \\textbf{W} = -\\epsilon \\delta \\textbf{x}_0$$\n","where $\\delta$ is the gradient (called `grad` in the following code; see the lecture material for its derivation), $ϵ$ is the learning rate, and $\\textbf{x}_0$ is our first input sample in variable `X0`.\n","\n","Recall that $y$ is the desired output, i.e. `y[0]` in this Python code, and $\\hat{y}$ is our predicted value called `y_out` here."]},{"cell_type":"code","metadata":{"id":"bs0EG3n9ve6J"},"source":["# the learning rate determines the step size in the gradient descent, you can experiment with different values if you want\n","learning_rate = 0.5\n","\n","# compute the gradient term\n","grad = (y_out - y[0]) * y_out * (1 - y_out)\n","\n","# Calculate the weight update\n","W_delta = - learning_rate * grad * X0\n","\n","print(\"W_delta = \", W_delta)\n","\n","# Update the weights\n","W += W_delta\n","print(\"Updated weights W = \", W)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ha1gJ0n9v2oV"},"source":["Let's try a forward propagation again with the same input."]},{"cell_type":"code","metadata":{"id":"fh40oHl2v6rs"},"source":["print(\"y_out_old = \", y_out) # let's print the values before the update\n","y_out = sigma(np.dot(W.T, X0))\n","\n","print('y_out =', y_out)\n","print('y[0] =', y[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lHc3os_AL54y"},"source":["You should notice that the result has moved (slightly!) towards the correct answer. In order to converge to the right value we have to perform more iterations!"]},{"cell_type":"markdown","metadata":{"id":"8INLy50fyORN"},"source":["### Q1: Training iterations **[TO COMPLETE]**\n","\n","Let's define a complete training procedure for our model. In each iteration we have to perform the forward propagation, then we'll check how much the output differs from the target and propagate the error back (backward propagation).  We'll do this for each sample data point and then iterate this over and over again using a for loop."]},{"cell_type":"markdown","source":["`%STARTCODE`"],"metadata":{"id":"pide_kEzSjEP"}},{"cell_type":"code","metadata":{"id":"PPXJwQm4ycgH"},"source":["# For the training we need to iterate over the dataset several times\n","num_iters = 1000\n","\n","# We'll also store the mean square error (MSE) in every round so we can see how it evolves\n","# mse is just an array to store these values at each round:\n","mse = np.zeros(num_iters)\n","\n","# Looping for the iterations\n","for it in range(num_iters):\n","    \n","    # For-loop going over each sample in X\n","    for n in range(len(X)):\n","        # Extract the n_th sample and the corresponding desired output\n","        x_n = np.reshape(X[n], (3,1))\n","        # Get the correponding target value\n","        y_target = y[n]\n","        \n","        # Forward propagation of the n_th sample\n","        y_out = sigma(np.dot(W.T, x_n))\n","\n","        # Let's keep track of the sum of squared errors\n","        #--------------------------------------------------------------------------\n","        mse[it] += # [TO COMPLETE] compute squared error between y_target and y_out\n","        #--------------------------------------------------------------------------\n","        # compute the gradient\n","        grad = (y_out - y_target) * y_out * (1 - y_out) \n","    \n","        # Calculate the weights update\n","        W_delta = - learning_rate * grad * x_n\n","  \n","        # Update the weights\n","        W += W_delta\n","    \n","    # Divide by the number of elements to get the mean of the squared errors\n","    mse[it] /= len(X)\n","\n","# Now let's see the output for each input sample with the trained weights\n","# Using batch mode (see next section) we can do this in a single line\n","print(\"Output after training, y_out =\")\n","y_out = sigma(np.dot(X, W))\n","print(y_out)\n","print(\"Target output, y =\")\n","print(y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`%ENDCODE`"],"metadata":{"id":"r4TFfm4tSmHl"}},{"cell_type":"markdown","source":["### Q2: Fourth sample **[TO COMPLETE]**\n","Why is the estimation for the fourth sample way different than its real label value? What adjustment can you consider to make this sample get classified correctly as well? (You do not have to implement the adjustment)"],"metadata":{"id":"pF3Gu_OpRwJh"}},{"cell_type":"markdown","source":["`%STARTEXT`"],"metadata":{"id":"0aSKWdX2S3TF"}},{"cell_type":"markdown","source":["Answer: **[TO COMPLETE]**"],"metadata":{"id":"nua5TaoySA0m"}},{"cell_type":"markdown","source":["`%ENDTEXT`"],"metadata":{"id":"61rcR2r-TBiV"}},{"cell_type":"markdown","metadata":{"id":"yT1qLBvBzpDc"},"source":["After the training phase, the output of the network is fairly close to the target output. \n","\n","\n","How many iterations were required in order to obtain this result? We have set the number of the iteration to $1000$, but it is interesting to investigate the trend of the error trought the training. In the next homework, we will discuss how to select the right number of iterations (also known as *epochs*), for now let's just plot its behaviour:"]},{"cell_type":"code","metadata":{"id":"a6RsKlA7zrw3"},"source":["plt.figure()\n","plt.plot(range(num_iters), mse, label=\"MSE\")\n","plt.xlabel(\"# Iterations\")\n","plt.title(\"MSE behaviour\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hiS6hr-eMWR1"},"source":["You should see the error going down pretty quickly in the beginning and then slowing down."]},{"cell_type":"markdown","metadata":{"id":"Zoz4y02rxorc"},"source":["### Batch training\n","\n","With real-world data it is unefficient to handle each example one-by-one like we did above. Instead, one typically uses a set, so called mini-batch, of several input examples at once.\n","\n","Let's consider a subset $\\tilde{\\textbf{X}} ⊆ \\textbf{X}$ of samples from the training set. Each of these samples is one row in $\\tilde{\\textbf{X}}$, instead of a single column vector as before. The forward propagation step looks a bit different mathematically: $\\hat{\\textbf{y}} = f(\\tilde{\\textbf{X}}\\textbf{W})$.\n","\n","Our whole dataset can be forward propagated without a for loop:"]},{"cell_type":"code","metadata":{"id":"pwc6hfIlxspG"},"source":["y_out = sigma(np.dot(X, W))\n","print(\"y_out =\", y_out)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H8pBN8voyLGv"},"source":["so we will get the corresponding output (each value in `y_out`) for each input (each row in `X`) in single matrix multiplication.  The error and weight updates can all be calculated in a single go, using matrix multiplications similarly to the steps we did above with single vectors.\n","\n","However, in these exercises we'll stick to looping over one sample at a time, as we will deal with the batch training mode in the next homeworks and in this case it does not lead to any significant speed advantage."]},{"cell_type":"markdown","metadata":{"id":"7wv8Vplo1F7E"},"source":["## Exercise 1.2: Two-layer Neural Network"]},{"cell_type":"markdown","metadata":{"id":"oiI7bIaY1OVI"},"source":["Now let's try a slightly more difficult example. Let's consider the following training set: \n","\n","||Input||         Output|\n","|:----:|:---:|:---:|---:|\n","| $x_1$|$x_2$|$x_3$| $y$|\n","| 0    | 0   | 0   | 1  |\n","| 0    | 0   | 1   | 1  |\n","| 0    | 1   | 0   | 1  |\n","| 0    | 1   | 1   | 0  |\n","| 1    | 0   | 0   | 1  |\n","| 1    | 0   | 1   | 0  |\n","| 1    | 1   | 0   | 1  |\n","| 1    | 1   | 1   | 1  |\n","\n","In particular, the new input-output configuration represents the following relation between three input vector components:\n","$$\n","y = \\text{NOT}((x_0 \\: \\text{XOR} \\: x_1) \\: \\text{AND} \\: x_2)\n","$$\n","\n","Where $x_0$, $x_1$ and $x_2$ correspond the input vector components, and $\\text{NOT}$, $\\text{XOR}$ and $\\text{AND}$ stands for the corresponding logical operations.\n","\n","As an example, coinsider the first sample (row), $x_0 \\: \\text{XOR} \\: x_1$ is $0$ and the $\\text{AND}$ operation between it and $x_3$ is again $0$. The negation of $0$ is $1$, which indeed has been reported as the target value for this sample.\n","\n","\n","This problem is interesting because it can not be solved by using a single layer perceptron. Indeed, you will need (at least) a two-layer network to solve it.\n","In this exercise we will first show that the network that we defined in the previous exercise can not solve this task, then we will define a two-layer Neural Network able to compute the correct solution."]},{"cell_type":"code","metadata":{"id":"MuSfKOpD1TqJ"},"source":["import numpy as np\n","import matplotlib.pyplot as plt \n","\n","np.random.seed([42])\n","\n","X = np.array([[0,0,0],\n","              [0,0,1],\n","              [0,1,0],\n","              [0,1,1],\n","              [1,0,0],\n","              [1,0,1],\n","              [1,1,0],\n","              [1,1,1]])\n","    \n","y = np.array([[1,1,1,0,1,0,1,1]]).T"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5FphvrH-1e5O"},"source":["As we did in the previous exercise, let's initialize the weights and define the activation function (also in this exercise we will use the sigmoid function)."]},{"cell_type":"code","metadata":{"id":"aMeTD8cb1kNm"},"source":["# Weights initialization\n","W = 2 * np.random.random((3,1)) - 1\n","\n","# Activation function\n","def sigma(x):\n","    return 1 / (1 + np.exp(-x) )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"unbPv9pn3kVE"},"source":["Now let's run the network defined in Exercise 1.1 to check whether it is able to solve the XOR problem."]},{"cell_type":"code","metadata":{"id":"PGlUPnvi31fL"},"source":["num_iters = 10000\n","learning_rate = 0.5\n","\n","for it in range(num_iters):\n","    for n in range(len(X)):\n","        x_n = np.reshape(X[n], (3,1))\n","        y_target = y[n]\n","        \n","        # Forward propagation\n","        y_out = sigma(np.dot(W.T, x_n))\n","\n","        # Compute the Gradient\n","        grad = (y_out - y_target)*y_out*(1 - y_out)\n","    \n","        # Calculate the weights update\n","        W_delta = -learning_rate * grad * x_n\n","\n","        # Update the weights\n","        W += W_delta\n","\n","\n","# Now let's see the output for each input sample with the trained weights\n","# Using batch mode we can do this in a single line\n","y_out = sigma(np.dot(X, W))\n","print(\"Output after training, y_out =\")\n","print(y_out)\n","print(\"Desired output, y = \")\n","print(y)\n","print(\"Difference = \", np.linalg.norm(y-y_out))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"myYbripS39R-"},"source":["### Q3: Two layers network [TO COMPLETE]\n","As you can see the network is not able to solve the problem, it's not even close! You can try to increase the number of iterations but it won't help (can you think of the reason for this behaviour?).\n","Let's add a single hidden layer, for example with 4 hidden nodes (you can try other numbers as well).\n","The input to the network is a vector $\\mathbf{x}$ as before.  The first hidden layer calculates $\\textbf{h} = \\sigma(\\textbf{W}_1^T\\mathbf{x})$ (note that now $\\textbf{W}_1 \\in \\mathbb{R}^{3 \\times 4}$).  The output layer computes $\\hat{y} = \\sigma(\\textbf{W}_2^T\\textbf{h})$. Remember that $\\hat{y}$ is called `y_out` in th code, while ${W}_2 \\in \\mathbb{R}^{3 \\times 1}$.\n","\n","As usual, we'll start by initializing the weights randomly:"]},{"cell_type":"code","metadata":{"id":"Llke92kx4MRR"},"source":["num_hidden = 4\n","\n","# initialize weights randomly with zero mean and uniformly distributed values in [-1,1]\n","W_1 = 2 * np.random.random((3,num_hidden)) - 1\n","W_2 = 2 * np.random.random((num_hidden,1)) - 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QnQfHhXV7dO0"},"source":["We have to define the training procedure in order to train the two-layers neural network:"]},{"cell_type":"markdown","source":["`%STARTCODE`"],"metadata":{"id":"IkOQXzfaTM-8"}},{"cell_type":"code","metadata":{"id":"slOBh8IJ4tSK"},"source":["num_iters = 2000\n","learning_rate = 0.5 # learning rate\n","\n","mse = np.zeros(num_iters)\n","\n","for it in range(num_iters):\n","    for n in range(len(X)):\n","        x_n = np.reshape(X[n], (3,1))\n","        y_target = y[n]        \n","        \n","        ## Forward propagation\n","        # Calculate h\n","        # [TO COMPLETE]\n","        \n","        # Calculate y_out\n","        # [TO COMPLETE]\n","        \n","        # Let's keep track of the sum of squared errors\n","        # [TO COMPLETE]\n","        \n","        # Compute the gradient\n","        # [TO COMPLETE]\n","        \n","        # Calculate the weight updates for W_1\n","        # hint: you can do this by performing a for loop over i (hidden nodes) and k (input nodes) and calculate \n","        # each W_1_ik update separately\n","        # [TO COMPLETE]\n","        \n","        # Update the weights, note: it's important the W weights are updated at the end,\n","        # the above calculation should be done with the old weights\n","        # [TO COMPLETE]\n","        \n","    # Divide by the number of elements to get the mean of the squared errors\n","    mse[it] /= len(X)\n","\n","# Compute output\n","y_out = sigma(np.dot(sigma(np.dot(X, W_1)), W_2))\n","print(\"Output after training, y_out =\")\n","print(y_out)\n","print(\"Desired output, y = \")\n","print(y)\n","print(\"Difference = \", np.linalg.norm(y-y_out))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`%ENDCODE`"],"metadata":{"id":"-0E6cB17TO7v"}},{"cell_type":"markdown","metadata":{"id":"1nYoyoR2fANg"},"source":["Now you should see outputs very similar to the desired ones!\n","Finaly, let's plot again the MSE behaviour:"]},{"cell_type":"code","metadata":{"id":"YKuG4wee9kIc"},"source":["plt.figure()\n","plt.plot(range(num_iters), mse, label=\"MSE\")\n","plt.xlabel(\"# Iterations\")\n","plt.title(\"Two-layers NN MSE behaviour\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cAmf-t0PurMP"},"source":["## Exercise 1.3: Handwritten digits classification\n","In this exercise, we try to apply what we learned in the previous exercise in a real-world scenario. In particular, we consider a simple digits classification problem. The model turns out to be similar to the perceptron implemented in Exercise 1.1, but here we will use softmax activation function and cross-entropy loss function. The idea is to create a model that has in input an image of a handwritten digit and that return a vector of 10 probabilities (one for each possible digit $0-9$). "]},{"cell_type":"markdown","metadata":{"id":"IAYb_cd8vkG-"},"source":["### Dataset\n","The dataset that we will use in this exercise is included in [scikit-learn](https://scikit-learn.org/stable/), one of the major Machine Learning libraries. The dataset is called `load_digits` and contains several hundreds of samples. Each datapoint is made of the handwritten digit image (or rather its $8\\times8$ pixel representation), that will be the input of our model, and the target digit value. \n","\n","Let's start by plotting one of this handwritten digit:"]},{"cell_type":"code","metadata":{"id":"MGrkAeWtufMk"},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.datasets import load_digits\n","\n","np.random.seed([42])\n","\n","digits = load_digits()\n","\n","def plot_digit(x, y):\n","  plt.figure(figsize=(6, 6))\n","  plt.imshow(x, cmap=plt.cm.gray_r,\n","            interpolation='nearest')\n","  plt.title(\"Image True Label: %d\" % y)\n","  plt.tick_params(bottom=False, left=False, labelbottom=False, labelleft=False)\n","  plt.show()\n","\n","sample_index = 42\n","plot_digit(digits.images[sample_index], digits.target[sample_index])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K0OsnNJbv69J"},"source":["It is better to check how an input in the dataset $\\mathbf{X}$ and its related target $\\mathbf{y}$ are represented in the dataset:"]},{"cell_type":"code","metadata":{"id":"ei9Uda4av5q2"},"source":["data = np.asarray(digits.images[sample_index], dtype='float32')\n","target = np.asarray(digits.target[sample_index], dtype='int32')\n","\n","np.set_printoptions(threshold=np.inf) # In this way we print the full array\n","print(\"X:\", data)\n","print(\"y:\", target)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# the dataset is the flattened version of all the images ( 8 x 8 = 64 values for 1797 images)\n","print(digits.data.shape)"],"metadata":{"id":"HfYJPxfQl7jL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["So essentialy the dataset is a matrix with the (color) values for each pixel and for each image, whereas the target is the digit itself."],"metadata":{"id":"T9yg07qT3-Kr"}},{"cell_type":"markdown","metadata":{"id":"8Hc97pziwLKa"},"source":["#### One-hot encoding\n","In order to have a representation of the target that will be similar to the output of the model (i.e. $\\hat{y}=0$ or $1$ for each of the 10 digits), we will use one-hot encoding. Basically, the one-hot encoding allow us to encode a categorical integer feature using a one-of-K scheme, where each class is translated to a specific index of an array."]},{"cell_type":"code","metadata":{"id":"eF6jEnOmwRUk"},"source":["def one_hot(n_classes, y):\n","    return np.eye(n_classes)[y]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For example, if there are 10 classes in total and a sample belongs to class number 3, we can translate the output to the following lenght-ten array of 0 and 1 (class 3 is in the fourth index because we start counting from zero!):"],"metadata":{"id":"onD0qqMx6shM"}},{"cell_type":"code","metadata":{"id":"r4CISmJ39lts"},"source":["one_hot(n_classes=10, y=3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gkSruP-bwkki"},"source":["### Q4: Activation Function - Softmax [TO COMPLETE]\n","As activation function we will use the Softmax function: this particular function is very useful when we have to deal with multiclassification tasks and one-hot target because it turns numbers, a.k.a. logits (pre-activations), into $m$ probabilities that sum to one. Basically, Softmax function outputs a vector that represents the probability distributions of a list of potential outcomes $j$:\n","$$\n","softmax(\\mathbf{x})_j = \\frac{e^{x_j}}{\\sum_{i=1}^{m}{e^{x_i}}}\n","$$"]},{"cell_type":"markdown","source":["`%STARTCODE`"],"metadata":{"id":"uLqbX30cTqbe"}},{"cell_type":"code","source":["#[TO COMPLETE] define the softmax function"],"metadata":{"id":"jRpVpZDGU2Cj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`%ENDCODE`"],"metadata":{"id":"L7ZOVS7bTsFN"}},{"cell_type":"markdown","metadata":{"id":"KP7xOmNqsE-R"},"source":["### Loss Function: Cross Entropy ###\n","Usually, a neural network classifier that use the softmax function in the final layer is trained using Cross-Entropy as loss function:\n","$$H(Y,P)=-E_{y \\sim Y}[log \\;P(y)]$$\n","where $Y$ and $P$ are the true and predicted labels distributions."]},{"cell_type":"code","metadata":{"id":"UYnPJ1lWsFkP"},"source":["EPSILON = 1e-8 # this is needed for numerical stability\n","\n","def cross_entropy(Y_true, Y_pred):\n","    Y_true, Y_pred = np.atleast_2d(Y_true), np.atleast_2d(Y_pred) # make sure the dimensions are right\n","    loglikelihoods = np.sum(np.log(EPSILON + Y_pred) * Y_true, axis=1)\n","    return -np.mean(loglikelihoods)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R2jR9ijztQWP"},"source":["### Weights Initailiazation\n","\n","Similarly to what we did in previous exercises, we have to initialize the weights but in this case we will consider the bias term as well. Therefore we define the weights $\\mathbf{W}\\in\\mathbb{R}^{m \\times n}$ and the bias $\\mathbf{b}\\in\\mathbb{R}^m$, where $n$ is the input size and $m$ is the number of classes.\n","Now we can define the output of our model as\n","\n","$$\\hat{\\mathbf{y}}=softmax(\\textbf{W} \\textbf{x}+\\mathbf{b})$$\n","\n"]},{"cell_type":"code","metadata":{"id":"FtgOtaVL-4xd"},"source":["np.random.seed([42])\n","\n","input_size = digits.data.shape[1]\n","n_classes = len(np.unique(digits.target))\n","\n","W = np.random.uniform(size=(input_size,n_classes), high=0.1, low=-0.1)\n","b = np.random.uniform(size=n_classes, high=0.1, low=-0.1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6Ab3_mFz-9kG"},"source":["Let's consider a sample from the training set, and plot the current output of our model before training it."]},{"cell_type":"code","metadata":{"id":"6VpIr1KY_CTd"},"source":["def plot_predictions(y_out, true_label):\n","  plt.bar(range(n_classes), y_out, label=\"Predictions\", color=\"red\")\n","  plt.ylim(0, 1, 0.1)\n","  plt.xticks(range(n_classes))\n","  plt.legend()\n","  plt.ylabel(\"Probability\")\n","  plt.xlabel(\"Digit Class\")\n","  plt.title(\"Image True Label: %d\" % true_label)\n","  plt.show()\n","\n","y_out = softmax(np.dot(digits.data[sample_index], W) + b)\n","plot_predictions(y_out, digits.target[sample_index])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We see that the most likely predictions for our _untrained_ model is just $0$ (or it could have been any other random guess) for the handwritten $1$."],"metadata":{"id":"LCdvwX-VjyeO"}},{"cell_type":"markdown","metadata":{"id":"r4DYTEEh_QDf"},"source":["### Q5: Training procedure [TO COMPLETE]\n","As in the previous exercise let's define a training procedure. Note that in this case, we have to compute the gradient according to the softmax function and the loss function that the training has to optimize. \n","\n","Hence, the gradient for the weights $\\textbf{W}$ is:\n","\n","$\\nabla_W=(\\mathbf{\\hat{y}}-\\mathbf{y}) \\cdot \\mathbf{x}$\n","\n","while for the bias is:\n","\n","$\\nabla_b=(\\mathbf{\\hat{y}}-\\mathbf{y})$\n","\n","During the training procedure let's also compute the accuracy of the predictions and the loss value at each iteration:\n","\n"]},{"cell_type":"markdown","source":["`%STARTCODE`"],"metadata":{"id":"QvqY4kDVx8Fq"}},{"cell_type":"code","metadata":{"id":"OcQfByoE_a6v"},"source":["num_iters = 50\n","learning_rate = 0.0005\n","\n","for it in range(num_iters):\n","    iteration_accuracy = []\n","    iteration_loss = []\n","    for i, (X, y) in enumerate(zip(digits.data, digits.target)):\n","\n","        # implement forward propagation\n","        y_out = # [TO COMPLETE]\n","\n","        # compute the error\n","        # [TO COMPLETE]\n","\n","        # compute the gradient\n","        # [TO COMPLETE]\n","\n","        # update the weights\n","        # [TO COMPLETE]\n","        \n","        iteration_accuracy.append(np.argmax(y_out) == y)\n","        iteration_loss.append(cross_entropy(one_hot(n_classes,y), y_out))\n","\n","    print(f\"iteration: {it}, -- accuracy: {np.mean(np.asarray(iteration_accuracy)):.2%}, -- loss: {np.mean(iteration_loss):.4f}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`%ENDCODE`"],"metadata":{"id":"mo3tIVERyAUv"}},{"cell_type":"markdown","metadata":{"id":"QnJH8lWaxtc4"},"source":["As you can see during the training the accuracy increases after each iteration, while the loss function value progressively declines.\n","\n","Finally, let's check how the prediction capability of our model changes after the training:"]},{"cell_type":"code","metadata":{"id":"jGrzV9w6_lwz"},"source":["y_out = softmax(np.dot(digits.data[sample_index], W) + b)\n","plot_predictions(y_out, digits.target[sample_index])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, let's test our model on custom generated images whose size are $8 \\times 8$. As you can see, we can simply simulate the number drawing by considering some non-zero pixels among zero-valued pixels. For example, the following test case includes `number 2` inside, that can be seen from positions of non-zero pixel locations as well. "],"metadata":{"id":"eCED9tQGbgz_"}},{"cell_type":"code","source":["test_num = 2\n","px = 10\n","test_img = np.array([\n","                    [0.,  0.,  px,  px, px,  px,  0.,  0.],\n","                    [0.,  0.,  0.,  0., 0.,  px,  0.,  0.],\n","                    [0.,  0.,  0.,  0., 0.,  px,  0.,  0.],\n","                    [0.,  0.,  0.,  0., px,  px,  0.,  0.],\n","                    [0.,  0.,  0.,  0., px,  0.,  0.,  0.],\n","                    [0.,  0.,  0.,  px, 0.,  0.,  0.,  0.],\n","                    [0.,  0.,  px,  px, 0.,  0.,  0.,  0.],\n","                    [0.,  0.,  px,  px, px,  px,  0.,  0.]                   \n","])\n","plot_digit(test_img, test_num)\n","\n","test_img = test_img.flatten() # flatten the array to length (64, )\n","y_pred = softmax(np.dot(test_img, W) + b)\n","plot_predictions(y_pred, test_num)"],"metadata":{"id":"LaV-ENlYWQ-q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Q6: Experiment with different images  [TO COMPLETE]\n","\n","\n","You should try different numbers by changing the values in \"test_img\" array. You might observe and review the effect of `px` on the final estimation. Are you able to draw a number that is still recognizable for a human but not for the model?\n","\n","*Note: the score of question 'Q6' will not contribute to the overall score of the HW*"],"metadata":{"id":"imQEdCGDoJmT"}},{"cell_type":"markdown","source":["`%STARTCODE`"],"metadata":{"id":"O2QKuRy-T2QY"}},{"cell_type":"code","source":["test_num = # [TO COMPLETE] Number you choose to draw on the test_img grid\n","px = 10\n","test_img = np.array([\n","                    [0.,  0.,  0.,  0., 0.,  0.,  0.,  0.],\n","                    [0.,  0.,  0.,  0., 0.,  0.,  0.,  0.],\n","                    [0.,  0.,  0.,  0., 0.,  0.,  0.,  0.],\n","                    [0.,  0.,  0.,  0., 0.,  0.,  0.,  0.],\n","                    [0.,  0.,  0.,  0., 0.,  0.,  0.,  0.],\n","                    [0.,  0.,  0.,  0., 0.,  0.,  0.,  0.],\n","                    [0.,  0.,  0.,  0., 0.,  0.,  0.,  0.],\n","                    [0.,  0.,  0.,  0., 0.,  0.,  0.,  0.]                   \n","])\n","plot_digit(test_img, test_num)\n","\n","test_img = test_img.flatten() # flatten the array to length (64, )\n","y_pred = softmax(np.dot(test_img, W) + b)\n","plot_predictions(y_pred, test_num)"],"metadata":{"id":"NCtOx-yLn7E-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`%ENDCODE`"],"metadata":{"id":"lx1xjp9UT5C5"}}]}